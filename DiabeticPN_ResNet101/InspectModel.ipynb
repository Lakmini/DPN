{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# Root directory of the project\n",
    "ROOT_DIR = 'D:/research/DiabeticPN/ResNet50/Mask_RCNN-master'\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "from samples.DiabeticPN import custom\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "custom_WEIGHTS_PATH = \"D:/research/DiabeticPN/ResNet50/Mask_RCNN-master/logs/wound20181217T1228/mask_rcnn_wound_0015.h5\"  # TODO: update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           wound\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                207\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset directory\n",
    "custom_DIR = 'D:/research/DiabeticPN/DataSet/MaskRCNNData'\n",
    "# Inference Configuration\n",
    "config = custom.CustomConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# Only inference mode is supported right now\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 61\n",
      "Classes: ['BG', 'wound']\n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset\n",
    "dataset = custom.CustomDataSet()\n",
    "dataset.load_custom(custom_DIR, \"val\")\n",
    "\n",
    "# # Must call before using the dataset\n",
    "dataset.prepare()\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  D:/research/DiabeticPN/ResNet50/Mask_RCNN-master/logs/wound20181217T1228/mask_rcnn_wound_0015.h5\n",
      "Re-starting from epoch 15\n"
     ]
    }
   ],
   "source": [
    "# load the last model you trained\n",
    "# weights_path = model.find_last()[1]\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", custom_WEIGHTS_PATH)\n",
    "model.load_weights(custom_WEIGHTS_PATH, by_name=True)# load the last model you trained\n",
    "# weights_path = model.find_last()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ID: wound.Grade0_49_resized.jpg (2) D:/research/DiabeticPN/DataSet/MaskRCNNData\\val\\Grade0_49_resized.jpg\n",
      "Original image shape:  [300 300   3]\n",
      "Processing 1 images\n",
      "image                    shape: (1024, 1024, 3)       min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 14)               min:    0.00000  max: 1024.00000  int32\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n"
     ]
    }
   ],
   "source": [
    "image_id = random.choice(dataset.image_ids)\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "info = dataset.image_info[image_id]\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "                                       dataset.image_reference(image_id)))\n",
    "print(\"Original image shape: \", modellib.parse_image_meta(image_meta[np.newaxis,...])[\"original_image_shape\"][0])\n",
    "\n",
    "# Run object detection\n",
    "results = model.detect([image], verbose=1)\n",
    "#results = model.detect_molded(np.expand_dims(image, 0), np.expand_dims(image_meta, 0), verbose=1)\n",
    "\n",
    "# Display results\n",
    "r = results[0]\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "# Compute AP over range 0.5 to 0.95 and print it\n",
    "utils.compute_ap_range(gt_bbox, gt_class_id, gt_mask,\n",
    "                       r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "                       verbose=1)\n",
    "\n",
    "visualize.display_differences(\n",
    "    image,\n",
    "    gt_bbox, gt_class_id, gt_mask,\n",
    "    r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "    dataset.class_names, ax=get_ax(),\n",
    "    show_box=False, show_mask=False,\n",
    "    iou_threshold=0.5, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display predictions only\n",
    "# visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "#                             dataset.class_names, r['scores'], ax=get_ax(1),\n",
    "#                             show_bbox=False, show_mask=False,\n",
    "#                             title=\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Ground Truth only\n",
    "# visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n",
    "#                             dataset.class_names, ax=get_ax(1),\n",
    "#                             show_bbox=False, show_mask=False,\n",
    "#                             title=\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 [300 300   3]   AP: 0.00\n",
      "  1 [300 300   3]   AP: 0.90\n",
      "  2 [300 300   3]   AP: 0.50\n",
      "  3 [300 300   3]   AP: 0.80\n",
      "  4 [300 300   3]   AP: 0.80\n",
      "  5 [300 300   3]   AP: 0.20\n",
      "  6 [300 300   3]   AP: 0.30\n",
      "  7 [300 300   3]   AP: 0.50\n",
      "  8 [300 300   3]   AP: 0.50\n",
      "  9 [300 300   3]   AP: 0.43\n",
      " 10 [300 300   3]   AP: 0.60\n",
      " 11 [300 300   3]   AP: 0.80\n",
      " 12 [300 300   3]   AP: 0.15\n",
      " 13 [300 300   3]   AP: 0.00\n",
      " 14 [300 300   3]   AP: 0.00\n",
      " 15 [300 300   3]   AP: 0.00\n",
      " 16 [300 300   3]   AP: 0.00\n",
      " 17 [300 300   3]   AP: 0.05\n",
      " 18 [300 300   3]   AP: 0.00\n",
      " 19 [300 300   3]   AP: 0.06\n",
      " 20 [300 300   3]   AP: 0.00\n",
      " 21 [300 300   3]   AP: 0.20\n",
      " 22 [300 300   3]   AP: 0.00\n",
      " 23 [300 300   3]   AP: 0.05\n",
      " 24 [300 300   3]   AP: 0.20\n",
      " 25 [300 300   3]   AP: 0.00\n",
      " 26 [300 300   3]   AP: 0.40\n",
      " 27 [300 300   3]   AP: 0.00\n",
      " 28 [300 300   3]   AP: 0.00\n",
      " 29 [300 300   3]   AP: 0.40\n",
      " 30 [300 300   3]   AP: 0.00\n",
      " 31 [300 300   3]   AP: 0.40\n",
      " 32 [300 300   3]   AP: 0.00\n",
      " 33 [300 300   3]   AP: 0.10\n",
      " 34 [300 300   3]   AP: 0.40\n",
      " 35 [300 300   3]   AP: 0.50\n",
      " 36 [300 300   3]   AP: 0.50\n",
      " 37 [300 300   3]   AP: 0.50\n",
      " 38 [300 300   3]   AP: 0.60\n",
      " 39 [300 300   3]   AP: 0.50\n",
      " 40 [300 300   3]   AP: 0.80\n",
      " 41 [300 300   3]   AP: 0.20\n",
      " 42 [300 300   3]   AP: 0.00\n",
      " 43 [300 300   3]   AP: 0.00\n",
      " 44 [300 300   3]   AP: 0.00\n",
      " 45 [300 300   3]   AP: 0.30\n",
      " 46 [300 300   3]   AP: 0.00\n",
      " 47 [300 300   3]   AP: 0.40\n",
      " 48 [300 300   3]   AP: 0.60\n",
      " 49 [300 300   3]   AP: 0.40\n",
      " 50 [300 300   3]   AP: 0.00\n",
      " 51 [300 300   3]   AP: 0.70\n",
      " 52 [300 300   3]   AP: 0.70\n",
      " 53 [300 300   3]   AP: 0.50\n",
      " 54 [300 300   3]   AP: 0.20\n",
      " 55 [300 300   3]   AP: 0.00\n",
      " 56 [300 300   3]   AP: 0.40\n",
      " 57 [300 300   3]   AP: 0.00\n",
      " 58 [300 300   3]   AP: 0.60\n",
      " 59 [300 300   3]   AP: 0.60\n",
      " 60 [300 300   3]   AP: 0.00\n"
     ]
    }
   ],
   "source": [
    "def compute_batch_ap(dataset, image_ids, verbose=1):\n",
    "    APs = []\n",
    "    for image_id in image_ids:\n",
    "        # Load image\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        # Run object detection\n",
    "        results = model.detect_molded(image[np.newaxis], image_meta[np.newaxis], verbose=0)\n",
    "        # Compute AP over range 0.5 to 0.95\n",
    "        r = results[0]\n",
    "        ap = utils.compute_ap_range(\n",
    "            gt_bbox, gt_class_id, gt_mask,\n",
    "            r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "            verbose=0)\n",
    "        APs.append(ap)\n",
    "        if verbose:\n",
    "            info = dataset.image_info[image_id]\n",
    "            meta = modellib.parse_image_meta(image_meta[np.newaxis,...])\n",
    "            print(\"{:3} {}   AP: {:.2f}\".format(\n",
    "                meta[\"image_id\"][0], meta[\"original_image_shape\"][0], ap))\n",
    "    return APs\n",
    "\n",
    "# Run on validation set\n",
    "limit = 5\n",
    "APs = compute_batch_ap(dataset, dataset.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AP overa 61 images: 0.2909\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean AP overa {} images: {:.4f}\".format(len(APs), np.mean(APs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
